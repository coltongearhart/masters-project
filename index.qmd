---
title: Sample size rules of thumb for out-of-sample prediction quality in multiple regression
author:
  - name: Colton Gearhart
    email: gearhart.colton.work@gmail.com
    roles: "Author"
    affiliation:
      - id: mu
        name: Miami University
        department: Statistics
        city: Oxford
        country: USA
        url: https://miamioh.edu/cas/departments/statistics/index.html
  - name: John Bailer
    email: baileraj@miamioh.edu
    roles: "Faculty Advisor"
    affiliations:
      - ref: mu
abstract: |
  Determining how large of a sample is needed is an important step when planning studies; if too large, resources could be wasted and if too small, the usefulness of any results can be greatly affected. A key factor in choosing an appropriate sample size is the goal of the particular analysis. If explanation, as reflected by hypothesis tests of particular regression coefficients is the goal, there are many existing and different types of methods to help guide researchers with sample size calculations. However, if the goal of a study is prediction, then available methods are more limited. Through many different defined scenarios, a Monte Carlo simulation was performed to assess the predictive accuracy of multiple linear regression models that are trained on various sample sizes according to some previously proposed conventional rules. Scenarios were based on different combinations of the number of predictor variables and the degree of multicollinearity between them. Predictive accuracy was measured as the cross-validation root mean square error for predictions and also the squared simple correlation between predicted and observed responses for the holdout samples. Based off this simulation, a slight relationship was found between the sample size and predictive accuracy as measured by root mean square error. While results are general, systematically incorporating other aspects of a multiple linear regression analysis could provide more meaningful insights. Additionally, with the goal of precision in prediction, the subtle patterns do not provide enough evidence to give preference to any of the conventional sample size rules.
date: last-modified
bibliography: references.bib
appendix-cite-as: display
format:
  html:
    theme: flatly
    code-block-border-left: "#2C3E50"
    highlight-style: github
    df-print: kable
number-sections: true
title-block-banner: true
fig-cap-location: top
fig-align: left
---

## Introduction

An important aspect of nearly every study is determining how large of a sample is needed to support the goals of any analysis. Many things need to be considered when choosing the sample size; it is not as simple as just getting as large of a sample as possible. Researchers know that larger (representative) samples mean more power for hypothesis tests and smaller margin of errors for estimating parameters, and, by default, this will always be preferred to smaller samples. However, this clearly ignores costs and other important considerations. For example, in any study involving human or vertebrate animal subjects, ethical consideration impacts study sizing to make sure that no more participants are put at risk than is strictly necessary. While this is not a concern for observational studies, a common issue is the availability of resources. Data collection and analysis may come at a cost, whether that be time, money or computation time. In designed experiments, this leads to many sample size selections based on the allocated budget. All of these considerations are necessary, but they donâ€™t directly answer the question of why having an appropriately sized study is crucial.

Common goals for multiple linear regression (MLR) studies are to detect the presence of any effect amongst the entire set of predictors (so a global or omnibus test), to test the effect of a single predictor after taking into account the others, or to develop a predictive model for some continuous response as a function of inputs. For the first two goals, sample size analyses are typically performed in the context of statistical power. This means researchers want to have enough data so that they have a good chance of detecting an effect size that has scientific importance. However, the meaning of "enough" data or large enough sample size may differ for prediction where a specific level of precision in prediction might be the goal.

In terms of statistical power, if the sample size is too small, "an under-sized study can be a waste of resources for not having the capability to produce useful results, while an over-sized one uses more resources than are necessary" [@lenth2001]. From a prediction standpoint, insufficient sample sizes translate to models that do not perform well on new samples of data. In other words, the results are not generalizable, they only apply to the current sample [@barcikowski2012]. Interest in accurate prediction has only grown in recent years and MLR is often taught as one of the first supervised learning methods. Techniques such as cross-validation are already frequently used to assess prediction quality and select models, but can the sample size rules of thumb suggested for sizing studies for hypothesis testing also be used to help size a study for prediction? Determining if conventional sample size rules provide enough data to predict well in various scenarios is the main focus of the research presented here.

## Background and literature review for sample size determinations

Sample size calculations related to a power analysis, which are based off hypothesis tests of particular parameters in a model, are common and familiar [@maxwell2000]. This strategy represents one class of methods that help guide practitioners for sample size calculations. In this area, literature is well developed and there are many existing methods that cover a variety of MLR goals. Another popular type of recommendations for sample size comes from conventional rules of thumb, which are much more simplistic than conducting power analyses and are usually simple linear combinations of the number of parameters in a model.

### Standard power analyses

Formal sample size calculations are performed using non-central $t$ or $F$ distributions to detect the impact of a particular single variable or set of variables, respectively. A common goal for MLR studies is to detect the presence of any effect among the entire set of predictors. This corresponds to an omnibus test of the null hypotheses that none of $p$ predictor variables are needed, e.g. $H_0: \beta_1 = \ldots = \beta_p = 0$. The non-centrality parameters of these distributions are related to the value of the regression coefficients in the population, the $\beta$s.

When $p = 1$, this simplifies to the trivial case of a two-sided test on the slope parameter of a simple linear regression (SLR) model. Theoretical results for the minimum sample size $n$ that achieves some power can easily be obtained using a non-central $t$ distribution. The non-centrality parameter $\lambda$ represents the effect size, or in other words, $\lambda$ is the standardized difference between the null value of $\beta_1$ and some hypothesized nonzero value of $\beta_1$. If we posit some specific non-zero value of $\beta_1$, the distributions of the test statistic $t^*$ are known both under null and the alternative hypotheses (see appendix for a more formal, written out explanation). A probability statement can be set up (with a desired power, say 0.8, and a constant Type I error rate level, say $\alpha = 0.05$, where the only two unknowns are $n$ and $\lambda$. Because the latter is a function of $n$, the $n$ that yields this desired power for specified $\lambda$ can be obtained.

The single predictor variable case shown above can be generalized to MLR if $p = 2, 3, \ldots$. When this is the case, many more factors need to be considered, and each one adds another layer of complexity to the sample size calculations. This holds true when trying to test other hypotheses for MLR as well, such as the effect of an individual predictor. This task becomes quite difficult because the predictor variables are often correlated as well and this is part of the specification of effect sizes for sample size planning [@maxwell2000]. With all factors considered, explicit solutions to these sample size problems become very hard, if not impossible to obtain without introducing an unreasonable number of assumptions, which is why researchers often search for other ways to help size their study.

### Rules of thumb

One way to plan required sample sizes without using power calculations is by using some conventional sample size rules, which can be referred to as rules of thumb (RoT). In the context of MLR studies, many rules of thumb for determining the required sample sizes have been proposed. In general, they can be written as $n \ge g(p)$, where $g(p)$ is some function of the number of predictors variables. Some examples include $n \ge 15p$ [@stevens2001], which suggests that for each additional predictor variable being considered, 15 additional subjects are required and also $n \ge 30 + 10p$ is recommended by @knapp1989, which has a similar interpretation, except a base of 30 subjects is added.

These rules of thumb for sample size determinations in the place of power analyses allow for rapid determination of a study design using only the size of the predictor set without needing to specify relationships among the predictors or anticipated impact of the predictors on the response. One problem is that there often is not good justification or documentation for how these rules were derived. For example, @nunnally1978 recommendation of samples sizes upwards of 300 or 400 was geared towards developing a prediction equation that will cross-validate [@maxwell2000]; however, other authors are not as clear. Additionally, the majority of these RoT fail to consider some measure of effect size, as they are just functions of the number of predictors. This means that "they can only be effective at specific -- usually unknown -- effect sizes" [@barcikowski2012].

### Other methods

Outside of the straight-forward power analyses and the conventional rules, several authors have developed new ways to look at sample size problems. Many other methods exist, but only two are discussed here for demonstration.

A theoretical / formulaic approach to conduct a power analysis using the non-central F distribution was used by [@maxwell2000]. While it is a variant of the hypothesis testing sizing methods, it has an interesting strategy for simplifying complex sample size problem. This method is geared towards finding the required sample size for to have adequate power when testing the effect of a single predictor, after controlling for the other included predictors. Previous research has shown that it is possible to write the effect size as a function of various types of population correlation coefficients, such as the squared semipartial correlation for a particular $X$ variable or the overall $R^2$ between $Y$ and the set of $X$s, among others. However, these "approaches require more information that is available" [@maxwell2000] and researchers often donâ€™t have good intuitions about reasonable values for these complex population parameters. Because of this, the proposed method is based on *a priori* beliefs about plausible zero order-correlation values, which researchers are often much more confident about. Then by introducing a simplifying assumption of an exchangeable correlation structure (i.e. each of the predictors is assumed to have a common correlation with the response and also each predictor correlates to the others with a common value), sample size estimations can be made.

@kelley2003 also developed another approach that emphasized Accuracy In Parameter Estimation (AIPE). The main goal of this method is to help plan studies that are able to determine the direction of an effect, estimate the effect accurately and also reject null the hypothesis with adequate power simultaneously. The specific target for accuracy is to provide sample size recommendations that yield sufficiently narrow confidence intervals for estimated population regression coefficients [@kelley2003]. This is done by framing the margin of error for the confidence interval (CI) of interest in terms of a population variance $\sigma^2$. However, because this is usually unknown, the CI is actually based on the sample variance $s^2$. If by random chance this turns out to be larger than $s^2$, the resulting estimated interval will be wider than desired. To combat this, a tolerance level is introduced that represents the probability the interval will be wider than desired. As a result, larger sample sizes are needed than if some tolerance was not taken into consideration [@kelley2003]. In doing so, the expected width of the CI of interest is controlled.

### Prediction based sample size recommendations {#sec-pred-based-recs}

Relative to the amount and diversity of literature for power-based methods, sample size determination where the main goal is prediction is nowhere near as comprehensive. However, it has been asserted that "sample size will almost certainly have to be much larger for obtaining a useful prediction equation than for testing the statistical significance of the multiple correlation coefficient" [@maxwell2000]. Even though a sample size may provide adequate power, there is no guarantee that it will lead to models that are generalizable to future samples, which is more or less the main goal of predictive modelling. Of course, implicit in the challenge of prediction is the assumption that the model being used is correctly specified.

@knofczynski2007 provided guidelines for sample sizes when the main goal was to have accurate predictions; more specifically, they wanted to find "sample regression models that predict similarly to the population regression model". Here, "similar" is defined as a strong enough correlation between the predictions based off the sample and the population models. In other words, a sample size is deemed sufficient if a large enough proportion of the replications ($> 0.95$) had a correlation greater than 0.92 or 0.98 (for good and excellent prediction levels, respectively) between predicted $Y$s using population information $\hat{Y}_{\text{pop}} = \beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p$ and predicted Ys using sample information $\hat{Y}_{\text{sample}} = \hat{\beta}_0 + \hat{\beta}_1 X_1 + \cdots + \hat{\beta}_p X_p$. Monte Carlo simulations were performed under a variety of settings for the number of predictors, population correlation structures among these predictors and also the population between the dependent and set of independent variables. Given an anticipated $R^2_{YX}$ (the squared multiple correlation coefficient between $Y$ and the set of $X$s), results of these simulations showed as $R^2_{YX}$ decreases, $n$ increases exponentially; in other words, less correlated population data requires larger sample sizes for good predictions.

@barcikowski2012 proposed the Precision Efficacy Analysis for Regression (PEAR) method, whose primary goal is to make sure that sample regression models are generalizable to future samples. It does this by limiting the amount of cross-validity shrinkage $\epsilon$. This can be defined as $\epsilon = R^2 - R^2_C$, where $R^2_C$ is a measure of the effectiveness of a sample regression model when applied to other samples [@herzberg]. This quantity can be interpreted as "the difference between a regression modelâ€™s apparent validity, as measured by $R^2$, and its actual predictive cross-validity" [@darlington1990]. The PEAR method has been shown to better than numerous other methods that share the same goal of limiting cross-validity shrinkage, and new results suggest that it is also effective for providing regression coefficients with smaller standard errors.

## Methods (simulation experiment)

As noted in the Introduction, the goal of this paper is to study sample size recommendations for prediction in MLR. This is investigated using a simulation experiment where 5-fold cross validation was implemented to measure the out-of-sample prediction performance of models based on various sample sizes. This strategy more closely resembles what is done in practice, where a single sample needs to be used for both training and testing, rather than two distinct samples. Sample sizes were chosen according to some of the proposed RoT and three population characteristics (the total number of predictors, the number of active predictors, and the degree of multicollinearity between them) were manipulated to cover a wide range of validating scenarios. Models were fit in the context of observational studies, where there was not a design for the levels of the predictor variables.

### Conditions studied and data simulation process {#sec-sim-conditions}

The main factor of interest for the simulation described below was the sample sizes used for the 5-fold cross validation procedure. Because three other factors were manipulated (total number of predictors, number of active predictors, and degree of multicollinearity), comparison of prediction quality for different sample sizes needed to be made for fixed levels of the other factors. Sample sizes were chosen according to several proposed RoT, each of which had a different way of incorporating the number of predictors into their recommendations for $n$. The RoT considered are shown in the table below in @tbl-mlr-rot. Note that several of the justifications for these rules involve minimizing shrinkage (also called cross-validity shrinkage), $\epsilon$, as defined in @sec-pred-based-recs.

<!-- NOTE: find justification for other rules -->

| General rule: $n \ge g(p)$ | $n_{\text{train}}$ (e.g. with $p = 5$) | Justification for the rule                                                                                                                                                                                                                                                                                                | Citation                        |
|:----------------|:----------------|:----------------------|:----------------|
| $10p$                      | 50                                     | Informal survey of faculty about rules of thumb they consider for sizing MLR studies.                                                                                                                                                                                                                                     | Origin of this rule is unclear. |
| $15p$                      | 75                                     | Based on Park and Dudychaâ€™s (1974) tables, it was found that generally about 15 subjects per predictor provides reliable regression equations for prediction. This was found under the assumption of the squared population multiple correlation $\rho^2 = 0.5$, which is a reasonable guess for social science research. | @stevens2001                    |
| $8p + 50$                  | 90                                     | Provides power of approximately 80% or more for tests of a multiple correlation with a medium effect size, which can be defined as Cohenâ€™s $f^2 = R^2 / (1 - R^2) = 0.15$ or equivalently $R^2 = 0.13$ (note that @cohen2013 suggested these values in the context of behavioral sciences).                               | @green1991                      |
| $10.8p + 11.8$             | 66                                     | Provides sample sizes with the goal of minimizing prediction error (as measured by mean absolute error, MAE) for MLR models based on a random sample of a multivariate normal population.                                                                                                                                 | @sawyer1982                     |

: Rules of thumb for multiple regression {#tbl-mlr-rot}

The number of total predictors $p$ was set at 3, 6 and 9 (note that the set of $n$ were chosen to cover the entire range of sample size recommendations based on the selected RoT for the largest number of predictors $p = 9$). Only $q$ of which will be active predictors; the remaining $p-q$ predictors are noise variables (note that proportion of $q$ active predictors to $p$ total predictors, so-called $q$-ratios, were 1/3, 2/3 and 3/3 for each level of $p$; thus, for $p = 3$: $q = 1, 2, 3$; for $p = 6$: $q = 2, 4, 6$ for $p = 9$: $q = 3, 6, 9$). The degree of linear association between these sets of predictors were set at independent, low, medium-low and medium levels. Without loss of generality, correlation matrices were used when specifying the variance-covariance parameter $\Sigma_X$ of the multivariate normal distribution used to generate the design matrix. This means all the diagonal terms of $\Sigma_X$ were always set to one (i.e. a unit variance of one) and all off-diagonal values represent pairwise correlation coefficients between $X_i$ and $X_j$ ($i \ne j$). Additionally, all correlation coefficients $\rho$ were assumed to be non-negative for simplicity (i.e. $0 \le \rho \le 1$). When all predictors were independent, off diagonal correlation coefficients were set to zero. Low correlation coefficients were defined as being in the interval $(0, 0,2]$. Medium-low correlation coefficients were defined as being in the interval $[0.2, 0.4]$. Medium correlation coefficients were defined as being in the interval $[0.4, 0.6]$. Specific values for the off-diagonal terms were randomly generated from a uniform distribution with bounds corresponding to that of the respective multicollinearity level. High correlation coefficients (i.e. $\rho > 0.6$) were excluded to avoid extreme multicollinearity. In total, all possible combinations number of predictors, number of active predictors, and degree of multicollinearity defined a set of 36 unique validation scenarios for each sample size $n$ ($(p \times q\text{-ratio} \times \rho_{XX}) = (3 \times 3 \times 4) = 36$).

For a general scenario with $q$ active predictors, $p-q$ inactive predictors, some level of multicollinearity and $n$ observations, the first step in generating the sample data was to construct a proper / usable correlation matrix (i.e. symmetric and positive definite). Values for the simple correlation coefficients were randomly sampled from a uniform distribution with bounds based on the multicollinearity level and filled in so that the matrix was symmetric. Then the $n$ rows of the design matrix were generated by sampling $(X_1, \ldots, X_p)$ from a $\text{MVN}_p(\mu_X, \rho_{XX})$ where $\mu_X$ is the mean vector of the $p$ predictor variables. This leads to $Y \mid (X_1, \ldots, X_p) \sim \text{N}(\mu_{Y \mid X}, \sigma^2)$, where $\mu_{Y \mid X} = \beta_1 X_1 + \cdots + \beta_p X_p$. Several simplifying assumptions were made during the data generation process. $\mu_X$ was assumed to be a zero vector; $\mu_{Y \mid X}$ did not include an intercept term $\beta_0$; all active population regression coefficient $\beta_j$, $j = 1, \ldots, q$, were set to an arbitrarily chosen 0.2 and the remaining $\beta_j$, $j = q+1, \ldots, p$, were set to zero; and lastly the amount of error variation in the response was held constant at an arbitrarily chosen value $\sigma^2 = 1$. All of this can be summarized with the following population model:

$$Y_i = \beta_1 X_1 + \cdots + \beta_p X_p + \epsilon_i$$ {#eq-pop-model}

where $X \sim \text{MVN}_p(\mu_X = \boldsymbol{0}, \rho_{XX})$, $\beta_j = 0.2$ ($j = 1, \ldots, q$) remaining $\beta_j = 0$ ($j = q+1, \ldots, p$), and $\epsilon_i \overset{iid}\sim \text{N}(0, \sigma^2 = 1)$.

### Predictions evaluated {#sec-preds-evals}

During the cross-validation procedure, each model was trained, and predictions were made on the holdout set of observations (see outlined steps in @sec-implementation). The quality of these out-of-sample predictions were judged on two accuracy measures: root mean squared error ($RMSE$ for prediction) and out of sample $R^2$ (@eq-rmse and @eq-oos-r2 below). These allowed for accuracy to be assessed from two different perspectives. $RMSE$ is a distance metric that represents the square root of the average squared prediction error. Relative to a large $RMSE$, a smaller $RMSE$ means that on average, predicted responses are closer to the true values. Thus, the goal is to minimize $RMSE$. It is calculated as follows:

$$\text{RMSE} = \bigg[\frac{1}{n_{\text{test}}} \sum^{n_{\text{test}}}_{i = 1} (y_{\text{new, }i} - \hat{y}_{\text{new, }i})^2\bigg]^{1/2}$$ {#eq-rmse}

Out-of-sample $R^2$ is a measure of the closeness of predicted responses to observed responses in a test data set based upon a model derived from distinct training data. Larger values of $R^2$ indicate that there is a stronger association between the predicted responses and the actual responses. Out-of-sample $R^2$ can is calculated by:

$$R^2_{\text{test}} = \text{Cor}(y_{\text{new}}, \hat{y}_{\text{new}})^2$$ {#eq-oos-r2}

### Implementation details {#sec-implementation}

Below are the steps used to conduct the Monte Carlo simulation within R; this outlined procedure was performed for each iteration of the simulation (in total, 500 iterations were run for each scenario).

Step 1) Get sample of data.

a.  For a particular number of predictors $p$ and level of multicollinearity, generate the correlation matrix.

b.  Once a proper matrix is obtained, simulate $n$ observations for $X$ utilizing $\rho_{XX}$.

c.  Set coefficients for $q$ active predictors to a constant, arbitrarily chosen value of $\beta_j = 0.2, \hspace{10pt} j = 1, \dots, q$ and non active predictor coefficients to zero $\beta_j = 0, \hspace{10pt} j = q + 1, \dots, p$. Then simulate the response variable $Y$ for each of the $n$ observations according to the population model in @eq-pop-model.

-   Note that all the above tasks for step 1 should be carried out following the descriptions in @sec-sim-conditions.

Step 2) Perform 5-fold cross validation.

a.  Partition sample data from step 1 into 5 groups (aka folds).

b.  Fit a sample MLR model using only data from four of the folds (e.g. folds 1 - 4).

c.  Using the regression equation from the above model, obtain the predicted values for observations in the fold that was held out (in this example, fold 5).

d.  Calculate the two accuracy measures described in @sec-preds-evals, @eq-rmse and @eq-oos-r2.

e.  Repeat steps b - d until each fold has been used as the testing set exactly once.

f.  Average the accuracy measures over the 5 folds.

Step 3) Go through each scenario.

-   Repeat steps 1 -- 2, each time using a different combination of the manipulated factors (i.e. number of predictors, number of active predictors, degree of multicollinearity and sample size). In other words, cycle through all of the 36 validation scenarios for each sample size. All of the calculated accuracy measures from this process make up the results for a single iteration of the simulation.

Once the entire simulation procedure was complete (steps 1 -- 3, for each iteration), accuracy measures were summarized over the 500 iterations, within each unique scenario.

## Results and conclusions

The results below in the figures below are based on the mean of the respective accuracy measures over the 500 iterations.

![Plots of RMSE by sample size when all predictors are active (q-ratio = 3/3) with sample size rules of thumb overlaid](files/plot-rmse-focused-annotated.png){#fig-rmse-focused}

The driving factors of the differing $RMSE$ values were sample size $n$ and the number of predictors $p$. In all three levels of predictors, there is a downward trend in $RMSE$ as $n$ increases. This pattern hints towards an asymptote for the minimum $RMSE$ that can be achieved, and it is achieved quicker with smaller $p$. This suggests that smaller sample sizes and larger predictors sets encounter overfitting, which is to be expected. It is important to note that the decreasing trend is somewhat magnified in @fig-rmse-focused because of the scale; there is only approximately a 0.2-unit change in average $RMSE$ for $p = 9$ between the minimum and maximum sample sizes that were tested (for perspective, $\sigma = 1$ for the population model). A zoomed-out plot of this can be found in @sec-appendix.

It can also be noted from @fig-rmse-focused that $RMSE$ values across all sample sizes for different multicollinearity levels for a given number of predictors $p$ were nearly identical, which is consistent with theory stating that prediction power is not affected by multicollinearity [@kutner]. This serves in part to validate the setup of the simulation. Additionally, trends and $RMSE$ values for $q$-ratio's of 1/3 and 2/3 were identical to that of 3/3 shown in @fig-rmse-focused (these plots can be seen in @sec-appendix). This may be because the effect size between $Y$ and the set of $X$s was left uncontrolled, and population $\beta$ values for the active predictors were set a constant, arbitrarily chosen value of 0.2 (for reference, the population error variance was $\sigma = 1$).

Taking all of this into account, the very slight decreasing trend and the apparent leveling off, it is not possible to say if one RoT would provide significantly better predictions than any other rule in terms of average prediction error magnitude.

Results based on $R^2_{Test}$ are shown below in @fig-r2-focused.

{{< embed notebooks/simulation.qmd#fig-r2-focused >}}

It can be seen that an increase in multicollinearity levels causes an inflation in $R^2_{Test}$ across all level of predictors and $q$-ratios. Additionally, this inflation is much more pronounced when there are more active predictors (as shown by the gaps in the 3/3 $q$-ratio). Similar trends were found in the $q$-ratio of 2/3 (not shown), just less pronounced. As expected, increasing the number of predictors steadily increases the $R^2_{Test}$. However, the downward trends shown when there is a $q$-ratio of 1/3 and for some of the multicollinearity levels with all active predictors were not anticipated and go against intuition. With larger sample sizes, one would expect the intermediate models fit during the cross-validation procedure to have more and more accurate predictions, on average. This would then carry through when averaging over the folds and the many iterations, ultimately leading to larger estimates for the testing $R^2$. However, that is not what happened here; more investigation is needed to find and understand the cause. Because of this uncertainty, sample sizes recommendations geared to this accuracy measure will not be shown or discussed.

Overall, when quantifying quality of predictions with $RMSE$, results showed only small changes when increasing the sample size from 30 to 300. This may be because certain aspects aspects of the simulation process with MLR models were left uncontrolled like some measure of effect size between $Y$ and the set of $X$s. Despite this, there was some noticeable patterns and trends between $RMSE$ and sample sizes (for a given level of multicollinearity and the number of predictors) that merit further investigation. Perhaps these relationships will be more pronounced when looked across even more general simulation settings. However, based on the implemented scenarios of this simulation, larger sample sizes did not see enough of a benefit in accuracy (i.e. smaller $RMSE$) to warrant the large increases in $n$. Thus, by default preference would be given to RoT that recommend smaller sample sizes when the goal of a MLR study is to have accurate predictions. Lastly, due to the suspicious pattern, nothing can be gleaned from the results when using out-of-sample $R^2$ as the measure of predictive accuracy.

## Future work

Results presented were fairly general because numerous situations were explored in terms of the number of predictors and the relationship between them. However, effect size was not controlled. Regardless of how it is defined, effect size is a key part in any analysis. Results would likely be more applicable if it was taken into account. Literature has shown that recommended sample size is a decreasing function of effect size. One potential way to control effect size is to use standardized regression models. By using a correlation transformation on both the $Y$ and $X$ data, these models allow the practitioner to directly specify the pairwise simple correlations between the response and each individual predictor variable. This enables population regression coefficients to be expressed as a function of this correlation vector. It is suspected that other methods for controlling the strength of the response to predictor relationship also involve the specification of the population $\beta$s.

Another avenue to extend this research would be to explore generalized linear models (GLM), such as logistic or Poisson regression. This would require finding new ways to define predictive accuracy that are appropriate for these different types of regression.

## References {.unnumbered}

::: {#refs}
:::

## Appendix {#sec-appendix}

{{< embed notebooks/simulation.qmd#fig-all-rmse >}}
